

	
	
	
	
	
		
	
	
	
	
	
		
	
	

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Richard Jeffrey - Subjective Probability: The Real Thing - Reviewed by Branden Fitelson, University of California, Berkeley - Philosophical Reviews - University of Notre Dame</title>
<link rel="stylesheet" type="text/css" href="css/global.css" title="Default Style" />
<link rel="stylesheet" type="text/css" media="print" href="css/print.css" />
<link rel="alternate stylesheet" type="text/css" media="screen" href="css/nostyle.css" title="No Style" />
<script type="text/javascript" src="js/styles.js"></script>
</head>
<body id="reviews">
<div id="wrapper">
  <div id="header">
		<a href="./" id="prhome">Philosophy Reviews</a>
    <ul id="menu">
		  <li><a href="reviews.cfm" id="rev">Reviews</a></li>
		  <li><a href="board.cfm" id="edi">Editorial Board</a></li>
      <li><a href="guidelines.cfm" id="gui">Reviewers Guidelines</a></li>
      <li><a href="http://listserv.nd.edu/cgi-bin/wa?SUBED1=philosophical-reviews&A=1" id="sub">Subscribe</a></li>
    </ul>
		<form action="search.cfm" method="get" id="search" title="Search Form">
			Search 
			  <select name="type">
          <option value="author">Author</option>
          <option value="title">Title</option>
          <option value="reviewer">Reviewer</option>
          <option value="keyword">Keyword</option>
        </select>
for	
<input name="q" type="text" size="16" />	
<input type="submit" name="Submit" value="Go" class="submit" />
		</form>
	</div>
  <div id="content">
    <div id="review"><h1>2005.10.15</h1>
    <h4>Richard Jeffrey</h4>
	<h2>Subjective Probability: The Real Thing</h2>
	
	<p class="biblio">Richard Jeffrey, <em>Subjective Probability: The Real Thing</em>, Cambridge University Press, 2004, 140 pp, $21.99 (pbk), ISBN 0521536685.</p>
	<p><strong>Reviewed by Branden Fitelson, University of California, Berkeley</strong></p>
    <div id="hr"><hr /></div>
	
	<p class="MsoBodyText">Richard Jeffrey was one of the all-time greats in formal epistemology, and this was his last book.<span style="">  </span>In classic Jeffrey style, what we have here is a short, dense, and incredibly rich and engaging monograph.<span style="">  </span>It is simply amazing how much wisdom is packed into this little book.<span style="">  </span><o:p></o:p></p>   <p class="MsoBodyText">Before getting down to Serious Bayesian Business, Jeffrey begins with an extended acknowledgements section, which contains a heartfelt, emotional, and informatively autobiographical letter of farewell and thanks.<span style="">   </span>The letter is addressed to "Comrades and Fellow Travelers in the Struggle for Bayesianism", and its author is introduced to the reader as "a fond foolish old fart dying of a surfeit of Pall Malls".<span style="">  </span>As someone who only barely knew Dick Jeffrey (but hopes to be a Comrade in the aforementioned Struggle when he grows up), I was deeply touched and inspired by this introductory section of the book.<span style="">  </span>It's no wonder that he was so beloved and respected both as a philosopher and as a man.<span style="">  </span><o:p></o:p></p>   <p class="MsoBodyText">The first chapter provides an excellent introduction to the basic concepts of subjective probability theory.<span style="">  </span>Both the formal probability calculus as well as its interpretation in terms of betting quotients for rational agents (the main application discussed in the book) are clearly and concisely presented here.<span style="">  </span>This includes very accessible and clear explanations of<span style="">  </span>"Dutch Book" arguments, conditional probability, and Bayes's Theorem.<span style="">  </span>There are many useful exercises, and (as always) plenty of wise remarks and notes along the way.<span style="">  </span>Jeffrey's style is highly effective pedagogically, because he tends to introduce things using snappy examples.<span style="">  </span>Only after whetting the reader's appetite with such examples does Jeffrey invite the reader to think more systematically and theoretically.<span style="">  </span>As such, this chapter would be a suitable (maybe even ideal) way to start an advanced undergraduate course on probability and induction (or inductive logic).<span style="">  </span>Indeed, I plan to try it myself the next time I teach such a course.<o:p></o:p></p>   <p class="MsoBodyText">Chapter two explains how subjective probability can be used to provide an account of the <i>confirmation</i><span style="font-style: normal;"> of scientific theories.<span style="">  </span>The basic idea is to model inductive learning (typically, involving observation) as an event (called an </span><i>update</i><span style="font-style: normal;">) that takes the agent from an </span><i>old</i><span style="font-style: normal;"> subjective probability assignment to a </span><i>new</i><span style="font-style: normal;"> one.<span style="">  </span>If this learning process leads to a greater probability of a hypothesis (</span><i>H</i><span style="font-style: normal;">) -- </span><i>i.e</i><span style="font-style: normal;">., if </span><i>new</i><span style="font-style: normal;">(</span><i>H</i><span style="font-style: normal;">) > </span><i>old</i><span style="font-style: normal;">(</span><i>H</i><span style="font-style: normal;">) --  then </span><i>H</i><span style="font-style: normal;"> is said to have been </span><i>confirmed</i><span style="font-style: normal;"> (presumably, by whatever was learned during the update).<span style="">  </span>Here,<span style="">  </span>Jeffrey uses examples from the history of science to frame the discussion.<span style="">  </span>Historical illustrations of both the Duhem-Quine problem and the problem of old evidence are treated here (I will return to Jeffrey's discussion of the problem of old evidence later in this review).<span style="">  </span>In keeping with Jeffrey's pedagogical style, no precise theory of updating is developed at this stage (although some hints and puzzles are presented, which naturally lead the reader into wondering how such a theory might go).<span style="">  </span>At this point, we just see some basic concepts applied to some simple historical examples.<span style="">  </span>Precise theories of probabilistic update are discussed in the next chapter.<span style="">  </span>From a pedagogical point of view, I suggest thinking of chapters two and three as operating together (I suspect that some students might have trouble following the details of the accounts exemplified in chapter two without delving into some of the more theoretical material in chapter three along the way).<o:p></o:p></span></p>   <p class="MsoBodyText">In chapter three we get a masterful primer on the two main Bayesian theories of learning (probabilistic update). The classical theory of <i>conditionalization</i><span style="font-style: normal;"> (in which learning is modeled as </span><i>conditionalizing</i><span style="font-style: normal;"> on a proposition explicitly contained in the agent's doxastic space), as well as Jeffrey's more general theory of </span><i>probability kinematics</i><span style="font-style: normal;"> (in which learning is modeled as an event that alters an agent's credence function, but not necessarily by explicit conditionalization on a proposition) are compared and contrasted in a very illuminating way.<span style="">  </span>We also get a pithy presentation of Jeffrey's "radical probabilist" epistemology, which was the philosophical motivation for his generalization of classical Bayesian conditionalization.<span style="">  </span>There are two main reasons why Jeffrey saw a need to generalize classical conditionalization.<span style="">  </span>First, classical conditionalization assumes that all learning is learning </span><i>with certainty</i><span style="font-style: normal;">, since, whenever we conditionalize on a proposition </span><i>E</i><span style="font-style: normal;">, we must subsequently assign </span><i>E</i><span style="font-style: normal;"> probability 1.<span style="">  </span>Second, classical conditionalization presupposes that there is always a </span><i>statement</i><span style="font-style: normal;"> (in the agent's mentalese) that expresses the precise content of what was learned during an update.<span style="">  </span>Jeffrey conditionalization weakens both of these assumptions, thereby providing a more general (and more "radically probabilistic") framework for learning.<span style="">  </span>The theoretical and philosophical aspects of this framework are laid out in chapter three.<o:p></o:p></span></p>   <p class="MsoBodyText">Before moving on to chapters four and five (which have to do with foundations and applications of subjective probability in statistics), I would like to digress with a few critical remarks on Jeffrey's account of the problem of old evidence presented in chapter two.<span style="">  </span>The problem of old evidence is a problem (first articulated by Clark Glymour) for the traditional Bayesian theory of confirmation which takes <i>conditionalization</i><span style="font-style: normal;"> as its learning rule.<span style="">  </span>According to this classical approach, </span><i>new</i><span style="font-style: normal;">(</span><i>H</i><span style="font-style: normal;">) = </span><i>old</i><span style="font-style: normal;">(</span><i>H</i><span style="font-style: normal;"> | </span><i>E</i><span style="font-style: normal;">), and </span><i>E </i><span style="font-style: normal;">confirms </span><i>H</i><span style="font-style: normal;"> iff </span><i>old</i><span style="font-style: normal;">(</span><i>H</i><span style="font-style: normal;"> | </span><i>E</i><span style="font-style: normal;">) > </span><i>old</i><span style="font-style: normal;">(</span><i>H</i><span style="font-style: normal;">).<span style="">  </span>Hence, once </span><i>E</i><span style="font-style: normal;"> is learned, it cannot confirm any hypothesis </span><i>thereafter</i><span style="font-style: normal;">, since all </span><i>subsequent</i><span style="font-style: normal;"> probability functions will have to assign probability 1 to </span><i>E</i><span style="font-style: normal;"> [</span><i>i.e.</i><span style="font-style: normal;">, </span><i>new</i><span style="font-style: normal;">(</span><i>E</i><span style="font-style: normal;">) = 1, and so </span><i>new</i><span style="font-style: normal;">(</span><i>X</i><span style="font-style: normal;"> | </span><i>E</i><span style="font-style: normal;">) = </span><i>new</i><span style="font-style: normal;">(</span><i>X</i><span style="font-style: normal;">) for all </span><i>X</i><span style="font-style: normal;">, and no </span><i>subsequent</i><span style="font-style: normal;"> confirmation of </span><i>any X</i><span style="font-style: normal;"> by </span><i>E</i><span style="font-style: normal;"> is possible].<span style="">  </span><o:p></o:p></span></p>   <p class="MsoBodyText">But, intuitively, there seem to be cases in which we <i>do</i><span style="font-style: normal;"> want to say that </span><i>E</i><span style="font-style: normal;"> confirms </span><i>H</i><span style="font-style: normal;"> </span><i>even though we have already learned</i><span style="font-style: normal;"> </span><i>E</i><span style="font-style: normal;">.<span style="">  </span>For instance, Einstein knew about (</span><i>E</i><span style="font-style: normal;">) the anomalous advance of the perihelion of Mercury, many years before he formulated his theory of general relativity (</span><i>H</i><span style="font-style: normal;">) which predicts it.<span style="">  </span>Nonetheless, it seems reasonable for Einstein to have judged that </span><i>E</i><span style="font-style: normal;"> confirms </span><i>H</i><span style="font-style: normal;"> (in 1915) when he learned that </span><i>H</i><span style="font-style: normal;"> predicts </span><i>E</i><span style="font-style: normal;">.<span style="">  </span>But a classical Bayesian theory of confirmation </span><i>cannot</i><span style="font-style: normal;"> undergird his claim.<span style="">  </span>[Many Bayesians respond to this problem by saying that, while Einstein's </span><i>actual</i><span style="font-style: normal;"> credence function in 1915 did not undergird the desired confirmation claim, some </span><i>historical</i><span style="font-style: normal;"> or </span><i>counterfactual</i><span style="font-style: normal;"> credence function does (</span><i>e.g.</i><span style="font-style: normal;">, the credence function he would have had, if he had not learned about the perihelion data).<span style="">  </span>I will not discuss such approaches here.]<span style="">  </span>Dan Garber provided a clever alternative explanation of confirmational judgments in such cases.<span style="">  </span>Garber suggested that, while </span><i>E</i><span style="font-style: normal;"> did not confirm </span><i>H</i><span style="font-style: normal;"> for Einstein in 1915, the fact that </span><i>H</i><span style="font-style: normal;"> </span><i>entails</i><span style="font-style: normal;"> </span><i>E</i><span style="font-style: normal;"> (which Einstein </span><i>did</i><span style="font-style: normal;"> learn in 1915) did.<span style="">  </span>The idea here is to model Einstein as an agent who is </span><i>not</i><span style="font-style: normal;"> </span><i>logically omniscient</i><span style="font-style: normal;">.<span style="">  </span>Garber does this by adding a new statement to our (sentential) probability model of Einstein's epistemic state.<span style="">  </span>This statement gets </span><i>extrasystematically</i><span style="font-style: normal;"> interpreted as "</span><i>H</i><span style="font-style: normal;"> </span><i>entails</i><span style="font-style: normal;"> </span><i>E</i><span style="font-style: normal;">".<span style="">  </span>Garber then assumes that Einstein has </span><i>some</i><span style="font-style: normal;"> knowledge about this entailment relation (that if </span><i>X </i><span style="font-style: normal;">is true and "</span><i>X</i><span style="font-style: normal;"> entails </span><i>Y" is true</i><span style="font-style: normal;">, then </span><i>Y</i><span style="font-style: normal;"> must also be true), but he does not know whether or not "</span><i>H</i><span style="font-style: normal;"> </span><i>entails</i><span style="font-style: normal;"> </span><i>E</i><span style="font-style: normal;">" is true.<span style="">  </span>Then, one can give constraints (historically plausible ones, even) on Einstein's credence function which ensure that "</span><i>H</i><span style="font-style: normal;"> </span><i>entails</i><span style="font-style: normal;"> </span><i>E</i><span style="font-style: normal;">" confirms </span><i>H</i><span style="font-style: normal;"> in the classical Bayesian sense.<o:p></o:p></span></p>   <p class="MsoBodyText"><span style="">  </span>Jeffrey speaks approvingly about this Garberian approach to "logical learning" and old evidence in chapter two.<span style="">  </span>But he then goes on to sketch an alternative account based on Jeffrey conditionalization.<span style="">  </span>On Jeffrey's account (which is rather tersely presented in chapter two), we assume that there are two learning events: the <i>empirical</i><span style="font-style: normal;"> update in which </span><i>E</i><span style="font-style: normal;"> is learned, and the </span><i>logical</i><span style="font-style: normal;"> update in which "</span><i>H</i><span style="font-style: normal;"> </span><i>entails</i><span style="font-style: normal;"> </span><i>E</i><span style="font-style: normal;">" is learned.<span style="">  </span>Jeffrey places various constraints on these two updates so as to ensure that, at the end of the two updates, </span><i>H</i><span style="font-style: normal;"> has a greater probability than it did before the two updates.<span style="">  </span>Thus, </span><i>H</i><span style="font-style: normal;"> is confirmed by the combination of the empirical and logical updates.<span style="">  </span>There are lots of moving parts and assumptions in Jeffrey's account (it's considerably more complex than Garber's conditionalization approach).<span style="">  </span>I won't get into these details here (although I think some of these assumptions are rather worrisome).<span style="">  </span>Rather, I'd like to focus on the </span><i>motivation</i><span style="font-style: normal;"> for a Jeffrey-conditionalization approach in the first place (in light of Garber's elegant, pre-existing classical conditionalization approach).<span style="">  </span>Recall the two motivations (in general) for abandoning strict conditionalization in favor of Jeffrey conditionalization: (1) that sometimes learning is </span><i>not</i><span style="font-style: normal;"> learning with certainty, and (2) sometimes there is no statement in the agent's mentalese that expresses what was learned.<span style="">  </span>The first motivation (1) cannot be relevant here, since (a) </span><i>E</i><span style="font-style: normal;"> </span><i>must</i><span style="font-style: normal;"> be learned with certainty in order for the problem of old evidence to get off the ground (if </span><i>E</i><span style="font-style: normal;"> is </span><i>not</i><span style="font-style: normal;"> learned with certainty, then </span><i>E</i><span style="font-style: normal;"> </span><i>can</i><span style="font-style: normal;"> still confirm </span><i>H</i><span style="font-style: normal;"> in the classical Bayesian sense, and there is </span><i>no</i><span style="font-style: normal;"> problem -- this is why </span><i>even Jeffrey</i><span style="font-style: normal;"> models the empirical update as a </span><i>strict</i><span style="font-style: normal;"> conditionalization), and (b) there is no reason to suppose that "</span><i>H</i><span style="font-style: normal;"> </span><i>entails</i><span style="font-style: normal;"> </span><i>E</i><span style="font-style: normal;">" is </span><i>not</i><span style="font-style: normal;"> learned with certainty here (and even if there were, it is unclear how that would help to resolve the problem anyway). <span style=""> </span>So, whatever Jeffrey sees as lacking in Garber's approach, it must have something to do with (2).<span style="">  </span><o:p></o:p></span></p>   <p class="MsoBodyText">But Jeffrey concedes that <i>E</i><span style="font-style: normal;"> </span><i>is</i><span style="font-style: normal;"> expressed by a statement in the agent's (sentential) mentalese (namely, "</span><i>E</i><span style="font-style: normal;">").<span style="">  </span>So, it seems that the </span><i>only</i><span style="font-style: normal;"> motivation for using Jeffrey conditionalization rather than strict conditionalization to model logical learning (and to use this logical learning to account for the old evidence problem à la Garber) is the worry that "</span><i>H entails E</i><span style="font-style: normal;">" is not expressed by any statement in the agent's mentalese.<span style="">  </span>Indeed, Jeffrey seems to presuppose this in his account sketched in chapter two.<span style="">  </span>I don't find this a very compelling worry.<span style="">  </span>After all, Garber has shown how to use </span><i>extrasystematic</i><span style="font-style: normal;"> </span><i>interpretation</i><span style="font-style: normal;"> of one of the sentences of the agent's language to model an agent's learning "</span><i>H entails E</i><span style="font-style: normal;">".<span style="">  </span>One might respond on behalf of Jeffrey by complaining that having a sentence which is </span><i>extrasystematically</i><span style="font-style: normal;"> interpreted as "</span><i>H entails E</i><span style="font-style: normal;">" is not the same thing as having a statement that </span><i>systematically</i><span style="font-style: normal;"> expresses "</span><i>H entails E</i><span style="font-style: normal;">".<span style="">  </span>That's true, but I don't see why it's a problem for Garber's approach.<span style="">  </span>It is quite common in the context of classical Bayesian confirmation theory to extrasystematically interpret statements in a sentential language as having first-order logical content which outstrips their systematic (propositional-logical) content.<span style="">  </span>For instance, in Bayesian approaches to the ravens paradox, (atomic) sentences in simple languages are extrasystematically interpreted as monadic first-order claims like "All ravens are black", and some of the (extrasystematic!) logical implications of these extrasystematic interpretations are </span><i>crucial</i><span style="font-style: normal;"> for proving the requisite "theorems" about the probability models in question.<span style="">  </span>So, unless there is some reason to think that such applications of classical Bayesian confirmation theory (which trace back to the origins of the discipline) need to be re-worked Jeffrey-style, so as to avoid the use of such "extrasystematic interpretations", I don't see why Garber's approach needs to be re-worked Jeffrey-style either.<span style="">  </span>That said, I think Jeffrey's approach to old evidence and logical learning is both novel and clever.<span style="">  </span>I just wonder whether its extra complexity and assumptions are really warranted, in light of Garber's simpler, classical approach.<o:p></o:p></span></p>   <p class="MsoBodyText">Chapter four contains a perspicuous and sophisticated introduction to the concept of <i>expectation</i><span style="font-style: normal;">, and its relation to </span><i>probability</i><span style="font-style: normal;">.<span style="">  </span>Both unconditional and conditional expectation are expertly (and accessibly) covered here, along with their (sometimes subtle) connections to unconditional and conditional probability.<span style="">  </span>This is something we (unfortunately) rarely see in a book on the philosophy of subjective probability.<span style="">  </span>But, it is essential for a thorough understanding of the foundations of the subject (especially as they were developed by de Finetti and others in the 20th century).<span style="">  </span>In particular, the basics of expectation are prerequisite for grasping a key concept discussed in chapter five: </span><i>exchangeability</i><span style="font-style: normal;">.<span style="">  </span>Exchangeability is considered by many to be the single most important concept in the foundations of subjective probability.<span style="">  </span>But it is almost never discussed in introductory texts on probability and inductive logic (at least, those that philosophers are likely to read).<span style="">  </span>In chapter five, Jeffrey provides a survey of some of the central results involving the concept of exchangeability.<span style="">  </span>The most important of these are various forms of and variations on de Finetti's </span><i>representation theorem</i><span style="font-style: normal;"> for subjective probability, which provides a key to unlocking the mystery of how subjective probabilities can be obtained (non-capriciously) by updating on statistical information.<span style="">  </span>This is some of the most technically (and philosophically) challenging material in the book.<span style="">  </span>But this chapter (especially) repays a careful work-through.<span style="">  </span>I would say that the material in this chapter will be most challenging for students (even those with some background in probability).<span style="">  </span>I would also say that those interested in the relationship between subjective and objective probability (</span><i>e.g.</i><span style="font-style: normal;">, probability in statistical mechanics) will find this chapter very illuminating and thought provoking (many references to excellent related work in statistics and physics are included here).<span style="">  </span>Those who want a deep understanding of the foundations of subjective probability and its relationship to contemporary statistical science would be well served by a careful study of chapters four and five of </span><i>The Real Thing</i><span style="font-style: normal;">. <o:p></o:p></span></p>   <p class="MsoBodyText">Chapter six (the final chapter of the book) is all about Jeffrey-style rational decision theory.<span style="">  </span>Here, the reader will find a very effective crash course on the basics of the theory of rational decision first outlined in Jeffrey's classic essay <i>The Logic of Decision</i><span style="font-style: normal;">.<span style="">  </span>The presentation here benefits from many years of reflection since the publication of </span><i>The Logic</i><span style="font-style: normal;">.<span style="">   </span>In the very final section of the book (to my mind, one of the most interesting and sophisticated sections therein), we hear a completely new take from Jeffrey on the Newcomb problem.<span style="">  </span>The Newcomb problem has plagued decision theorists (especially those of Jeffrey's ilk) for over thirty-five years.<span style="">  </span>Here, at the very end of his very last work, Jeffrey renounces much of what he had been saying about that thorny problem for many years.<span style="">  </span>In the process, he provides many wonderful new insights and ideas.<span style="">  </span>This is the mark of a great philosophical mind (or, in his words, "a fond foolish old fart").<span style="">  </span>Even the last pages of his last book involve radical re-workings of age-old resolutions of the deepest philosophical puzzles.<span style="">  </span>Richard Jeffrey was one of the greatest philosophers of probability, induction, and rational decision we have known.<span style="">  </span>His last book has given me a healthy dose of his wisdom.<span style="">  </span>May it do the same for you.<span style="">  </span><o:p></o:p></span></p>   

	
  </div></div>
  
<div id="footer">
    <p>Copyright &copy; 2004 Notre Dame Philosophical Reviews<br />
  Site Last Modified: 4/10/2011	<br>
ISSN: 1538 - 1617</p>
    <p><a href="http://www.nd.edu/" id="ndhome">University of Notre Dame </a></p>
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-344381-6";
urchinTracker();
</script>
</div>

</div>
</body>
</html>
