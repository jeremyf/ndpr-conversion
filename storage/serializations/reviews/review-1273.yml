--- 
catalog_id: 2003.05.07
bibliography: Searle, John, <em>Consciousness and Language</em>, Cambridge University Press, 2002, 278pp, $23.00 (pbk), ISBN 0521597447
images: []

review_id: 1273
content: "<p>The fourteen essays collected in this book\xE2\x80\x94most of them already published\xE2\x80\x94cover a variety of topics that John Searle has been concerned with over twenty years, from language, conversation, and speech-act theory to consciousness, cognition, and the indeterminacy of translation. As a whole, the book offers many stimulating views, and some of the most controversial should spark new interdisciplinary reflections. Chapter 10, \xE2\x80\x9CHow performatives work\xE2\x80\x9D, presents a fascinating discussion of how declarations are encoded. The analysis of self-referentiality in promises that is offered in this chapter is a great piece of philosophical theorizing. The present review will concentrate on the main topic of the book: consciousness and its role in cognition.</p>  \t\t \n\
  <p>Consciousness and intentionality, the author contends, are essentially biological phenomena, which might at first blush seem to imply that no artificial device can ever think or become conscious of the world. But the view is, rather, that a computational representation offering a simulation of the <i>logical</i> relations between the subset of brain states that are the vehicles of mental representations would not qualify as a possible candidate for conscious agency. For such a simulation ignores that the relevant dimension for conscious awareness cannot be information processing (the argument as will be shown below, relies on Searle\xE2\x80\x99s particular view as to what information consists in); what is relevant to consciousness is rather a specific set of \xE2\x80\x9Cbiological\xE2\x80\x9D processes in the brain that produces it.</p>  \t\t \n\
  <p>Although, as Searle acknowledges, philosophy has nothing to say about the particular biological process in question, it is reasonable to assume that conceptual clarification will play a major role in any solution of the problem of consciousness. The author insists, in particular, on the ontological status of consciousness: it is caused by the brain, but is also a \xE2\x80\x9Cfeature of the brain\xE2\x80\x9D: \xE2\x80\x9Cconsciousness is a state that the brain is in\xE2\x80\x9D (48). When the causal regularities that govern such a realization are properly understood, a duplication (not a simulation) of conscious states in artifacts might be considered.</p>  \t\t \n\
  <p>Searle anticipates the objection that conscious states have a first-person, i.e. a qualitative, experiential ontology, in contrast with the third-person biological and physical phenomena studied in science. How then could a causal, third-person approach possibly clarify what consciousness is? His rejoinder consists in developing a new kind of ontological stance, called \xE2\x80\x9Cbiological naturalism\xE2\x80\x9D, that keeps both dualism and materialism at bay. Dualism is notoriously unable to account for the causal connection between the mental and the physical. Materialism, on the other hand, assumes that all existing phenomena are physical; it is therefore unable to acknowledge, and still less to account for, the existence of subjective qualitative states. A proper ontology should, according to Searle, recognize that subjective facts of consciousness can be endowed with epistemic objectivity, which allows them to constitute <i>bona fide</i> objects of scientific inquiry (23). One might object that the difficulty of traditional dualism would surface again in the two varieties of objective facts: granting that the real world encompasses epistemically subjective (mental) facts and third-person (inter alia: cerebral) facts, the difficulty persists in understanding how they can be both accounted for in one and the same explanatory framework.</p>  \t\t \n\
  <p>On this issue, a crucial element in Searle\xE2\x80\x99s strategy is to contrast causation and reduction: conscious states cannot be <i>reduced</i> to lower-level properties of the brain \xE2\x80\x93 otherwise the felt quality is lost; they can only be \xE2\x80\x9Ccausally explained\xE2\x80\x9D (34). Now one might want to object that causal explanation standardly understood is offered in a detached, third-person, non-qualitative way. Why should a statement such as \xE2\x80\x9CX produces C\xE2\x80\x9D, where X refers to biological facts and C to a felt quality, be taken to provide a causally adequate explanation (causally adequate to the <i>subjective</i> \t\t\t<i>explanandum)</i>? This objection, associated in the literature with the \xE2\x80\x9Chard problem\xE2\x80\x9D, is for Searle just an expression of our present limited understanding of which range of facts are relevant (24-25). This admission however backfires on the whole epistemological theory and on the underlying ontology; what is needed is an indication as to how the causal statement above can be validated in principle; this problem cannot be solved simply through the discovery of the relevant neural correlations.</p>  \t\t \n\
  <p>Chapter 7 presents in a summarized form the arguments offered in the two final chapters of <i>The Rediscovery of the Mind</i> \t\t\t<sup><a name=\"1b\" href=\"#1\">1</a></sup> against the informational paradigm as developed in cognitive science. In a nutshell, the line of reasoning is this. Any kind of causal explanation has to cite \xE2\x80\x9Creal features of the real world\xE2\x80\x9D\xE2\x80\x94this requirement is called the \xE2\x80\x9Ccausal reality constraint\xE2\x80\x9D (107). Any appeal to informational, subdoxastic states, however, fails to meet this constraint, according to Searle, because in this kind of case, information is observer-relative rather than intrinsic. Observer-relativity in turn allows interpretive free-wheeling: \xE2\x80\x9Cany system of any complexity at all admits of an information processing analysis\xE2\x80\x9D (p. 110). This latter feature, for Searle, shows that information might not be playing any causal role in observer-relative cases. Only (potentially or actually) conscious states qualify as intrinsic states, able to have a causal role qua mental.</p>  \t\t \n\
  <p>Such a claim clashes with a view predominant in work presently conducted in artificial intelligence, experimental psychology, cognitive linguistics and cognitive neuroscience. In these fields, it is widely assumed that information processing is one of the most prominent functions of the brain, whether their associated mental content is in principle available to the agent or not. It is worthwhile trying to answer Searle\xE2\x80\x99s objections, because they point to genuine conceptual difficulties.</p>  \t\t \n\
  <p>What is information? And why should it be taken to be observer-relative in all cases in which the subject is attributed a mental state but is not in a position to report consciously about it? In Searle\xE2\x80\x99s use of the notion, information is an <i>epistemic</i> notion; in other words, information is present in a state of affairs (tree rings, neuronal state) <i>only if</i> a thinker is able to read it off. On this view of information, it is a matter of definition that an informational state is conscious (in fact or in principle\xE2\x80\x94as for example, in a conscious perceptual state, or in a belief, an intention or a desire); thus information can be said to play a causal role because it is part of the intentional content available to the agent. In contrast, when applied from a third-person point of view (by the light of an observer), it does not meet the causal reality constraint.</p>  \t\t \n\
  <p>If the summary above is correct, it appears that Searle\xE2\x80\x99s objection to cognitive science is associated with his defining information in epistemic terms. As a result, any attempt to understand intentional states in terms of informational content\xE2\x80\x94which is the project of cognitive science\xE2\x80\x94is considered to be doomed to circularity. This is because you cannot aim at trying to reduce intentional states to informational states if you assume that beliefs and desires are needed to extract information.</p>  \t\t \n\
  <p>Another concept of information is used, however, in cognitive science. As Searle acknowledges (p. 119), \xE2\x80\x9Cin a perfectly reasonable, but a different meaning of \xE2\x80\x99information\xE2\x80\x99\xE2\x80\x9C, \xE2\x80\x9Dthese tree rings contain information about the age of the tree\xE2\x80\x9D can be rephrased as: \xE2\x80\x9Cthere is an exact covariance between the number of the tree\xE2\x80\x99s rings and its age in years\xE2\x80\x9D. This is a non-epistemic concept of information. Having this non-epistemic concept is crucial for a cognitive scientist or a philosopher like Dretske who wants to explain how intentional states can be generated from informational states. In that alternative sense of the term, it is perfectly possible to attribute an informational state to an organism in a third-person way and to take information to have a causal role in the observed behavior. The causal properties of informational states are made clear in the three conditions that, according to Dretske, have to be fulfilled for a physical state to have representational content: i) there must be a brain state that covaries with an external event or property; ii) this internal state must be systematically triggering a given response to that event (for example flight, or motivational change); and iii) this state must have the function of carrying that information: in other terms, the response must be connected in a causally structured way to the informational state in question. Dretske himself adds to these conditions a requirement on mental content that might at first blush look similar to Searle\xE2\x80\x99s. To qualify as <i>mental content</i>, Dretske says, information must be cognitively available to the subject and not simply present \xE2\x80\x9Cobjectively\xE2\x80\x9D in its receptors\xE2\x80\x99 relevant states. What he means here is that an organism qualifies as intentional only if the information that is extracted is available to it in some central way in order to control its responses. For Dretske, an animal able to learn new concepts and categories in order to cope with a changing environment would thus qualify as having intentional states. In Dretske\xE2\x80\x99s view, the ability to learn and to exert a global control of behavior through representations plays the role that consciousness plays in Searle\xE2\x80\x99s view as the essential feature of mentality. An area in which this divergence is put to the test is the issue of animal minds.</p>  \t\t \n\
  <p>Animals don\xE2\x80\x99t express their beliefs in a language, but, Searle maintains, they do have intentional states. Why? Chapter 4 offers two forms of defense: i) Because they are conscious beings; ii) \xE2\x80\x9CBecause they correct their beliefs all the time on the basis of their perceptions\xE2\x80\x9D (68). These two lines of argument, however, do not determine the same set of animal minds. Whether or not animals are \xE2\x80\x9Cconscious\xE2\x80\x9D may depend on various properties still under debate, for example whether they have a nervous system allowing them to have reafferent perceptions, or whether they are able to form metarepresentations about their sensory states. Whether or not they can use representations to control their behavior, on the other hand, depends on their ability to extract information, form categories, maintain them in memory over time, and apply them to new objects. There is no a priori guarantee, to say the least, that the capacity to feel and to experience, on the one hand, and to think, i.e. to represent the world in a structured way, on the other, define coextensive classes of beings.</p>  \t\t \n\
  <p>Chapter 4, in connection with chapter 7, raises another difficulty. As we saw earlier, Searle takes information to be, in itself, observer-relative, and thus not able to constitute a \xE2\x80\x9Creal feature of the real world\xE2\x80\x9D. But consciousness is attributed to other animals, Searle acknowledges, because of the overwhelming analogy between animals and humans in their needs and actions; third-person attribution in this case does not automatically prevent us from accessing a real feature of the world: \xE2\x80\x9CEven if we assume that there is no fact of the matter as to which is the correct translation of the dog\xE2\x80\x99s mental representations into our vocabulary, that by itself does not show that the dog does not have any mental representations, any beliefs and desires, that we are trying to translate\xE2\x80\x9D (66). It is surprising that Searle finds it unproblematic to attribute consciousness to animals from a human viewpoint \xE2\x80\x93 for if information is observer-relative, projection from human to animal consciousness is not only observer-relative, but also infected with anthropocentrism; as Nagel and McGinn have emphasized, there is no way to <i>know</i> \xC3\x82\xC2\xABwhat it is like\xC3\x82\xC2\xBB to be a bat, or even a dog, on the basis of a simple analogy with our own conscious experience. (Imagining is not the problem.)</p>  \t\t \n\
  <p>The difficulty with Searle\xE2\x80\x99s position is threefold. First, conscious states have been found, in cognitive psychology, <i>not to exhaust</i> the range of intentional states; in other words, there are more perceptions and beliefs that control behavior in an effective and global way than the subject can consciously recognize. A large part of learning occurs outside consciousness, and such extensive \xE2\x80\x9Cimplicit learning\xE2\x80\x9D cannot (\xE2\x80\x99in principle\xE2\x80\x99) be made conscious. Second, the agent is in no better position than an external interpreter to tell which perceptions, which beliefs and desires, have been effective in her selecting a particular course of action. Thus consciousness can be a poor guide for appreciating the causes of emotional states or of agency. Thirdly, more generally, conscious awareness cannot constitute a cause of behavior, assuming it does, independently of the structure of the information that is being used, contrary to what Searle maintains: \xE2\x80\x9CWhen Ludwig [the dog] wants to eat or wants to drink, for example, he need not use any symbols or sentences at all to have his canine desires. He just feels hungry or thirsty\xE2\x80\x9D (118). But how can consciousness be so serviceable? For his objection against a computational view of the mind to be effective, Searle has to offer a theory of thought in addition to his theory of consciousness.</p>  \t\t \n\
  <p>It may seem a matter of course that Ludwig does not catch a ball <i>because</i> he processes information, but rather <i>because</i> he <i>wants</i> to catch the ball. But the story for \xE2\x80\x9Cwanting\xE2\x80\x9D (and for \xE2\x80\x9Cbecause\xE2\x80\x9D) is a long one, involving the phylogeny of motivation, action and social interaction. Important steps in this selective history of the will would involve informational processes such as object-tracking, categorizing, weighing properties, selecting contexts, etc.. There may, therefore, not be two different kinds of causation involved in the sentence above. The fact that we, human, language-using beings, find it easier to take the personal perspective does not imply that a special kind of expertise is reserved for that level (see p. 123). The \xE2\x80\x9Csimply conscious\xE2\x80\x9D view can seem to exhaust explanation only if one chooses to apply a common-sense interpretation to a complex underlying process.<span style=\"font-weight: bold;\">\r\n\
  </span></p> <p style=\"font-weight: bold;\">Endnotes</p>  \t      \t\t <p class=\"endnotes\"><a name=\"1\" href=\"#1b\">1.</a> 1992, Cambridge, MIT Press.</p>"
links: 
- "#1"
- "#1b"
authors: John Searle
reviewer: Joelle Proust, Institut Jean-Nicod (CNRS, Paris)
transformed_content: "<p>The fourteen essays collected in this book\xE2\x80\x94most of them already published\xE2\x80\x94cover a variety of topics that John Searle has been concerned with over twenty years, from language, conversation, and speech-act theory to consciousness, cognition, and the indeterminacy of translation. As a whole, the book offers many stimulating views, and some of the most controversial should spark new interdisciplinary reflections. Chapter 10, \xE2\x80\x9CHow performatives work\xE2\x80\x9D, presents a fascinating discussion of how declarations are encoded. The analysis of self-referentiality in promises that is offered in this chapter is a great piece of philosophical theorizing. The present review will concentrate on the main topic of the book: consciousness and its role in cognition.</p>  \t\t \n\
  <p>Consciousness and intentionality, the author contends, are essentially biological phenomena, which might at first blush seem to imply that no artificial device can ever think or become conscious of the world. But the view is, rather, that a computational representation offering a simulation of the <i>logical</i> relations between the subset of brain states that are the vehicles of mental representations would not qualify as a possible candidate for conscious agency. For such a simulation ignores that the relevant dimension for conscious awareness cannot be information processing (the argument as will be shown below, relies on Searle\xE2\x80\x99s particular view as to what information consists in); what is relevant to consciousness is rather a specific set of \xE2\x80\x9Cbiological\xE2\x80\x9D processes in the brain that produces it.</p>  \t\t \n\
  <p>Although, as Searle acknowledges, philosophy has nothing to say about the particular biological process in question, it is reasonable to assume that conceptual clarification will play a major role in any solution of the problem of consciousness. The author insists, in particular, on the ontological status of consciousness: it is caused by the brain, but is also a \xE2\x80\x9Cfeature of the brain\xE2\x80\x9D: \xE2\x80\x9Cconsciousness is a state that the brain is in\xE2\x80\x9D (48). When the causal regularities that govern such a realization are properly understood, a duplication (not a simulation) of conscious states in artifacts might be considered.</p>  \t\t \n\
  <p>Searle anticipates the objection that conscious states have a first-person, i.e. a qualitative, experiential ontology, in contrast with the third-person biological and physical phenomena studied in science. How then could a causal, third-person approach possibly clarify what consciousness is? His rejoinder consists in developing a new kind of ontological stance, called \xE2\x80\x9Cbiological naturalism\xE2\x80\x9D, that keeps both dualism and materialism at bay. Dualism is notoriously unable to account for the causal connection between the mental and the physical. Materialism, on the other hand, assumes that all existing phenomena are physical; it is therefore unable to acknowledge, and still less to account for, the existence of subjective qualitative states. A proper ontology should, according to Searle, recognize that subjective facts of consciousness can be endowed with epistemic objectivity, which allows them to constitute <i>bona fide</i> objects of scientific inquiry (23). One might object that the difficulty of traditional dualism would surface again in the two varieties of objective facts: granting that the real world encompasses epistemically subjective (mental) facts and third-person (inter alia: cerebral) facts, the difficulty persists in understanding how they can be both accounted for in one and the same explanatory framework.</p>  \t\t \n\
  <p>On this issue, a crucial element in Searle\xE2\x80\x99s strategy is to contrast causation and reduction: conscious states cannot be <i>reduced</i> to lower-level properties of the brain \xE2\x80\x93 otherwise the felt quality is lost; they can only be \xE2\x80\x9Ccausally explained\xE2\x80\x9D (34). Now one might want to object that causal explanation standardly understood is offered in a detached, third-person, non-qualitative way. Why should a statement such as \xE2\x80\x9CX produces C\xE2\x80\x9D, where X refers to biological facts and C to a felt quality, be taken to provide a causally adequate explanation (causally adequate to the <i>subjective</i> \t\t\t<i>explanandum)</i>? This objection, associated in the literature with the \xE2\x80\x9Chard problem\xE2\x80\x9D, is for Searle just an expression of our present limited understanding of which range of facts are relevant (24-25). This admission however backfires on the whole epistemological theory and on the underlying ontology; what is needed is an indication as to how the causal statement above can be validated in principle; this problem cannot be solved simply through the discovery of the relevant neural correlations.</p>  \t\t \n\
  <p>Chapter 7 presents in a summarized form the arguments offered in the two final chapters of <i>The Rediscovery of the Mind</i> \t\t\t<sup><a name=\"1b\" href=\"#1\">1</a></sup> against the informational paradigm as developed in cognitive science. In a nutshell, the line of reasoning is this. Any kind of causal explanation has to cite \xE2\x80\x9Creal features of the real world\xE2\x80\x9D\xE2\x80\x94this requirement is called the \xE2\x80\x9Ccausal reality constraint\xE2\x80\x9D (107). Any appeal to informational, subdoxastic states, however, fails to meet this constraint, according to Searle, because in this kind of case, information is observer-relative rather than intrinsic. Observer-relativity in turn allows interpretive free-wheeling: \xE2\x80\x9Cany system of any complexity at all admits of an information processing analysis\xE2\x80\x9D (p. 110). This latter feature, for Searle, shows that information might not be playing any causal role in observer-relative cases. Only (potentially or actually) conscious states qualify as intrinsic states, able to have a causal role qua mental.</p>  \t\t \n\
  <p>Such a claim clashes with a view predominant in work presently conducted in artificial intelligence, experimental psychology, cognitive linguistics and cognitive neuroscience. In these fields, it is widely assumed that information processing is one of the most prominent functions of the brain, whether their associated mental content is in principle available to the agent or not. It is worthwhile trying to answer Searle\xE2\x80\x99s objections, because they point to genuine conceptual difficulties.</p>  \t\t \n\
  <p>What is information? And why should it be taken to be observer-relative in all cases in which the subject is attributed a mental state but is not in a position to report consciously about it? In Searle\xE2\x80\x99s use of the notion, information is an <i>epistemic</i> notion; in other words, information is present in a state of affairs (tree rings, neuronal state) <i>only if</i> a thinker is able to read it off. On this view of information, it is a matter of definition that an informational state is conscious (in fact or in principle\xE2\x80\x94as for example, in a conscious perceptual state, or in a belief, an intention or a desire); thus information can be said to play a causal role because it is part of the intentional content available to the agent. In contrast, when applied from a third-person point of view (by the light of an observer), it does not meet the causal reality constraint.</p>  \t\t \n\
  <p>If the summary above is correct, it appears that Searle\xE2\x80\x99s objection to cognitive science is associated with his defining information in epistemic terms. As a result, any attempt to understand intentional states in terms of informational content\xE2\x80\x94which is the project of cognitive science\xE2\x80\x94is considered to be doomed to circularity. This is because you cannot aim at trying to reduce intentional states to informational states if you assume that beliefs and desires are needed to extract information.</p>  \t\t \n\
  <p>Another concept of information is used, however, in cognitive science. As Searle acknowledges (p. 119), \xE2\x80\x9Cin a perfectly reasonable, but a different meaning of \xE2\x80\x99information\xE2\x80\x99\xE2\x80\x9C, \xE2\x80\x9Dthese tree rings contain information about the age of the tree\xE2\x80\x9D can be rephrased as: \xE2\x80\x9Cthere is an exact covariance between the number of the tree\xE2\x80\x99s rings and its age in years\xE2\x80\x9D. This is a non-epistemic concept of information. Having this non-epistemic concept is crucial for a cognitive scientist or a philosopher like Dretske who wants to explain how intentional states can be generated from informational states. In that alternative sense of the term, it is perfectly possible to attribute an informational state to an organism in a third-person way and to take information to have a causal role in the observed behavior. The causal properties of informational states are made clear in the three conditions that, according to Dretske, have to be fulfilled for a physical state to have representational content: i) there must be a brain state that covaries with an external event or property; ii) this internal state must be systematically triggering a given response to that event (for example flight, or motivational change); and iii) this state must have the function of carrying that information: in other terms, the response must be connected in a causally structured way to the informational state in question. Dretske himself adds to these conditions a requirement on mental content that might at first blush look similar to Searle\xE2\x80\x99s. To qualify as <i>mental content</i>, Dretske says, information must be cognitively available to the subject and not simply present \xE2\x80\x9Cobjectively\xE2\x80\x9D in its receptors\xE2\x80\x99 relevant states. What he means here is that an organism qualifies as intentional only if the information that is extracted is available to it in some central way in order to control its responses. For Dretske, an animal able to learn new concepts and categories in order to cope with a changing environment would thus qualify as having intentional states. In Dretske\xE2\x80\x99s view, the ability to learn and to exert a global control of behavior through representations plays the role that consciousness plays in Searle\xE2\x80\x99s view as the essential feature of mentality. An area in which this divergence is put to the test is the issue of animal minds.</p>  \t\t \n\
  <p>Animals don\xE2\x80\x99t express their beliefs in a language, but, Searle maintains, they do have intentional states. Why? Chapter 4 offers two forms of defense: i) Because they are conscious beings; ii) \xE2\x80\x9CBecause they correct their beliefs all the time on the basis of their perceptions\xE2\x80\x9D (68). These two lines of argument, however, do not determine the same set of animal minds. Whether or not animals are \xE2\x80\x9Cconscious\xE2\x80\x9D may depend on various properties still under debate, for example whether they have a nervous system allowing them to have reafferent perceptions, or whether they are able to form metarepresentations about their sensory states. Whether or not they can use representations to control their behavior, on the other hand, depends on their ability to extract information, form categories, maintain them in memory over time, and apply them to new objects. There is no a priori guarantee, to say the least, that the capacity to feel and to experience, on the one hand, and to think, i.e. to represent the world in a structured way, on the other, define coextensive classes of beings.</p>  \t\t \n\
  <p>Chapter 4, in connection with chapter 7, raises another difficulty. As we saw earlier, Searle takes information to be, in itself, observer-relative, and thus not able to constitute a \xE2\x80\x9Creal feature of the real world\xE2\x80\x9D. But consciousness is attributed to other animals, Searle acknowledges, because of the overwhelming analogy between animals and humans in their needs and actions; third-person attribution in this case does not automatically prevent us from accessing a real feature of the world: \xE2\x80\x9CEven if we assume that there is no fact of the matter as to which is the correct translation of the dog\xE2\x80\x99s mental representations into our vocabulary, that by itself does not show that the dog does not have any mental representations, any beliefs and desires, that we are trying to translate\xE2\x80\x9D (66). It is surprising that Searle finds it unproblematic to attribute consciousness to animals from a human viewpoint \xE2\x80\x93 for if information is observer-relative, projection from human to animal consciousness is not only observer-relative, but also infected with anthropocentrism; as Nagel and McGinn have emphasized, there is no way to <i>know</i> \xC3\x82\xC2\xABwhat it is like\xC3\x82\xC2\xBB to be a bat, or even a dog, on the basis of a simple analogy with our own conscious experience. (Imagining is not the problem.)</p>  \t\t \n\
  <p>The difficulty with Searle\xE2\x80\x99s position is threefold. First, conscious states have been found, in cognitive psychology, <i>not to exhaust</i> the range of intentional states; in other words, there are more perceptions and beliefs that control behavior in an effective and global way than the subject can consciously recognize. A large part of learning occurs outside consciousness, and such extensive \xE2\x80\x9Cimplicit learning\xE2\x80\x9D cannot (\xE2\x80\x99in principle\xE2\x80\x99) be made conscious. Second, the agent is in no better position than an external interpreter to tell which perceptions, which beliefs and desires, have been effective in her selecting a particular course of action. Thus consciousness can be a poor guide for appreciating the causes of emotional states or of agency. Thirdly, more generally, conscious awareness cannot constitute a cause of behavior, assuming it does, independently of the structure of the information that is being used, contrary to what Searle maintains: \xE2\x80\x9CWhen Ludwig [the dog] wants to eat or wants to drink, for example, he need not use any symbols or sentences at all to have his canine desires. He just feels hungry or thirsty\xE2\x80\x9D (118). But how can consciousness be so serviceable? For his objection against a computational view of the mind to be effective, Searle has to offer a theory of thought in addition to his theory of consciousness.</p>  \t\t \n\
  <p>It may seem a matter of course that Ludwig does not catch a ball <i>because</i> he processes information, but rather <i>because</i> he <i>wants</i> to catch the ball. But the story for \xE2\x80\x9Cwanting\xE2\x80\x9D (and for \xE2\x80\x9Cbecause\xE2\x80\x9D) is a long one, involving the phylogeny of motivation, action and social interaction. Important steps in this selective history of the will would involve informational processes such as object-tracking, categorizing, weighing properties, selecting contexts, etc.. There may, therefore, not be two different kinds of causation involved in the sentence above. The fact that we, human, language-using beings, find it easier to take the personal perspective does not imply that a special kind of expertise is reserved for that level (see p. 123). The \xE2\x80\x9Csimply conscious\xE2\x80\x9D view can seem to exhaust explanation only if one chooses to apply a common-sense interpretation to a complex underlying process.<span style=\"font-weight: bold;\">\r\n\
  </span></p> <p style=\"font-weight: bold;\">Endnotes</p>  \t      \t\t <p class=\"endnotes\"><a name=\"1\" href=\"#1b\">1.</a> 1992, Cambridge, MIT Press.</p>"
review_title: Consciousness and Language
