
















<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>Richard Swinburne (ed.) - Bayes's Theorem - Branden Fitelson , University of California'Berkeley - Philosophical Reviews - University of Notre Dame</title>
<link rel="stylesheet" type="text/css" href="css/global.css" title="Default Style" />
<link rel="stylesheet" type="text/css" media="print" href="css/print.css" />
<link rel="alternate stylesheet" type="text/css" media="screen" href="css/nostyle.css" title="No Style" />
<script type="text/javascript" src="js/styles.js"></script>
</head>
<body id="reviews">
<div id="wrapper">
  <div id="header">
    <a href="./" id="prhome">Philosophy Reviews</a>
    <ul id="menu">
      <li><a href="reviews.cfm" id="rev">Reviews</a></li>
      <li><a href="board.cfm" id="edi">Editorial Board</a></li>
      <li><a href="guidelines.cfm" id="gui">Reviewers Guidelines</a></li>
      <li><a href="http://listserv.nd.edu/cgi-bin/wa?SUBED1=philosophical-reviews&A=1" id="sub">Subscribe</a></li>
    </ul>
    <form action="search.cfm" method="get" id="search" title="Search Form">
      Search
        <select name="type">
          <option value="author">Author</option>
          <option value="title">Title</option>
          <option value="reviewer">Reviewer</option>
          <option value="keyword">Keyword</option>
        </select>
for
<input name="q" type="text" size="16" />
<input type="submit" name="Submit" value="Go" class="submit" />
    </form>
  </div>
  <div id="content">
    <div id="review"><h1>2003.11.10</h1>
    <h4>Richard Swinburne (ed.)</h4>
  <h2>Bayes's Theorem</h2>

  <p class="biblio">Swinburne, Richard (ed.), <em>Bayes's Theorem</em>, Oxford University Press, 2002, 160pp, $24.95 (hbk), ISBN 0197262678.</p>
  <p><strong>Reviewed by Branden Fitelson , University of California'Berkeley</strong></p>
    <div id="hr"><hr /></div>

<p>This is a high quality, concise collection of articles on the foundations of probability and statistics. Its editor, Richard Swinburne, has collected five papers by contemporary leaders in the field, written a pretty thorough and even-handed introductory essay, and placed a very clean and accessible version of Reverend Thomas Bayes’s famous essay (“An Essay Towards the Solving a Problem in the Doctrine of Chances”) at the end, as an Appendix (with a brief historical introduction by the noted statistician G.A. Barnard). I will briefly discuss each of the five papers in the volume, with an emphasis on certain issues arising from the use of probability as a tool for thinking about <i>evidence</i>.</p>  
<p>In the first essay, Elliott Sober contrasts Bayesian accounts of evidential support with an alternative, non-Bayesian, likelihood-based approach. The crux of Sober’s non-Bayesian proposal involves the following sort of claim about <i>contrastive</i> evidential support:</p>  <blockquote>Evidence <i>E</i> favors hypothesis <i>H</i><sub>1</sub> over hypothesis <i>H</i><sub>2</sub>.</blockquote>  
<p>Here, the alternative hypotheses <i>H</i><sub>1</sub> and <i>H</i><sub>2</sub> need not be mutually exclusive. Sober proposes that we should unpack this relational concept of favoring using <i>likelihoods</i>, as follows:</p>  <blockquote>Evidence <i>E</i> favors hypothesis <i>H</i><sub>1</sub> over hypothesis <i>H</i><sub>2</sub> if Pr(<i>E</i> | <i>H</i><sub>1</sub>) > Pr(E | <i>H</i><sub>2</sub>).</blockquote>  
<p>This principle is sometimes called the “Law of Likelihood” (see Royall (1997) for the history and theoretical basis of this “law”). From a Bayesian (and, I think, <i>intuitive</i> point of view), this “law” is far from obvious.  Consider a case in which <i>E</i> entails <i>H</i><sub>1</sub> but fails to entail <i>H</i><sub>2</sub>. Intuitively, in such a case, <i>E</i> should <i>favor H</i><sub>1</sub>over <i>H</i><sub>2</sub>. After all, E <i>guarantees</i> the truth of <i>H</i><sub>1</sub>, but fails to guarantee the truth of <i>H</i><sub>2</sub>.  It is important to note that the “Law of Likelihood” is <i>inconsistent</i> with this intuitive principle. That is, there can be cases in which Pr(<i>E</i> | <i>H</i><sub>1</sub>) > Pr(E | <i>H</i><sub>2</sub>), <i>despite</i> the fact that <i>E</i> entails <i>H</i><sub>2</sub> but fails to entail <i>H</i><sub>1</sub>. Of course, these will be cases in which <i>H</i><sub>1</sub> and <i>H</i><sub>2</sub> are <i>not</i> mutually exclusive, but a <i>likelihoodist</i> cannot object to such counterexamples on <i>these</i> grounds (since mutual exclusivity is <i>not</i> a requirement for the likelihoodist’s “favoring” relation). A proper, Bayesian theory of contrastive confirmation, on the other hand, need not have this undesirable consequence.</p>  
<p>Bayesians typically understand relational support in terms of <i>non</i>-contrastive <i>confirmation</i>. For a Bayesian, <i>E</i> supports (or confirms) <i>H</i> – in a <i>non</i>-contrastive sense – just in case <i>E</i> raises the probability of <i>H</i> (on a suitable, rational credence function). There have been various proposals concerning how a Bayesian ought to measure the <i>degree</i> to which <i>E</i> confirms <i>H</i>, or <i>c</i>(<i>H</i>, <i>E</i>), for short (see Fitelson (1998) for a survey). But, no matter which Bayesian <i>c</i>-measure one favors, one would be inclined to define <i>contrastive</i> support (or <i>favoring</i>) in terms of this <i>non</i>-contrastive confirmation measure <i>c</i>, as follows:</p>  <blockquote>Evidence <i>E</i> favors hypothesis <i>H</i><sub>1</sub> over hypothesis <i>H</i><sub>2</sub> if <i>c</i>(<i>H</i><sub>1</sub>, <i>E</i>) > <i>c</i>(<i>H</i><sub>2</sub>, <i>E</i>).</blockquote>  
<p>Interestingly, this “reduction” of contrastive support to non-contrastive (Bayesian) confirmation need not be at odds with the “Law of Likelihood”. As it turns out, there is one (and only one, out of all the historical proposals!) Bayesian measure of confirmation that <i>entails</i> the “Law of Likelihood”, assuming this standard Bayesian definition of favoring in terms of confirmation (see Milne (1996)). This is important, as it shows that the Bayesian need not reject the “Law of Likelihood.” However, those who think the “law” is false (like myself) would be forced either to abandon the reductive principle stated above, or to choose a different measure of non-contrastive confirmation. Indeed, many have opted for the latter approach. While I would recommend endorsing <i>both</i> the former approach <i>and</i> the latter approach, it is worth mentioning that the following <i>weakened</i> version of the “Law of Likelihood” should be acceptable to <i>all</i> parties here, Bayesian or otherwise:</p>  <blockquote>Evidence <i>E</i> favors hypothesis <i>H</i><sub>1</sub> over hypothesis <i>H</i><sub>2</sub> if</blockquote>  <blockquote>Pr(<i>E</i> | <i>H</i><sub>1</sub>) > Pr(E | <i>H</i><sub>2</sub>) <i>and</i> Pr(<i>E</i> | ¬<i>H</i><sub>1</sub>) ≤ Pr(E | ¬<i>H</i><sub>2</sub>).</blockquote>  
<p>Joyce (2003) shows that this principle is satisfied by <i>all</i> reductive Bayesian confirmation-theoretic approaches to favoring (that is, <i>all</i> Bayesian measures of confirmation <i>c</i> will lead to definitions of “favoring” that satisfy this weak likelihood principle). This is a nice way to see precisely where Bayesian and non-Bayesian accounts of evidential support come apart. Bayesians are perfectly happy to talk about the likelihoods of the <i>denials</i> of alternative hypotheses: Pr(E | ¬<i>H</i><sub>1</sub>) and Pr(E | ¬<i>H</i><sub>2</sub>). But, non-Bayesian Likelihoodists will not feel comfortable with such probabilities, since they involve <i>averaging</i> over the likelihoods of concrete alternative hypotheses. And, the “weights” in these averages will depend on the dreaded <i>prior probabilities</i> of the alternative hypotheses: Pr(<i>H</i><sub>1</sub>) and Pr(<i>H</i><sub>2</sub>). While Bayesians are happy to use priors in their account of evidential support, non-Bayesians like Sober are strongly opposed to such a move, since they think the prior probabilities are (in general) subjective and that they lack probative force. Ultimately, it seems to me, whether terms like Pr(<i>E</i> | ¬<i>H</i><sub>1</sub>) should be countenanced in our theory of evidence will depend on the overall relative adequacy of Bayesian <i>vs</i> non-Bayesian accounts of evidential support. Howson (pp. 52–53) argues in his contribution to this volume that such likelihoods are <i>crucial</i> for properly understanding evidential support. And, I am inclined to agree (see Fitelson (2001) for some further reasons why). Indeed, even <i>non</i>-Bayesians will use such terms <i>sometimes</i> — when it seems to be <i>essential</i> to obtaining the right answers about <i>contrastive</i> evidential support (see Royall (1997, pages 1–2), and even Sober (2003) for some clear examples of this kind).</p>  
<p>Sober’s paper concludes with a discussion of recent <i>instrumentalist</i>, non-Bayesian approaches to statistical inference. Here, he highlights the work of the Japanese statistician Akaike, which aims to show how the <i>simplicity</i> of a model can be tied to its <i>predictive accuracy</i>. This is a very important area of research in contemporary statistics and also in the philosophy of science. Sober argues that Bayesian approaches to these issues and problems cannot adequately account for the importance of simplicity as a factor in determining how predictively accurate a statistical model is. Howson, and other Bayesians, are usually not convinced by such arguments. And, in fairness to the Bayesian approaches, I think there is more that can be said on this score (for a nice Bayesian discussion of simplicity in this context, see Rosenkrantz (1977)). I conclude my discussion of Sober’s paper with a detail that the minute reader may find puzzling. In the first part of his paper, Sober talks about “favoring,” which, presumably, involves evidence favoring the <i>truth</i> of one hypothesis over another (not, say, favoring the <i>predictive accuracy</i> of one over another), but in the second part he talks only about comparative judgments of <i>predictive accuracy</i> and <i>not</i> about truth. It is unclear to me how the likelihoods appearing in Akaike’s theorem are to be interpreted. Are they still capturing what the evidence says about the <i>truth</i> of competing theories, or are they merely containing information relevant to assessing relative <i>predictive accuracy</i>? It is interesting that (either way) likelihoods would then seem to be essential <i>both</i> to the instrumentalist <i>and</i> to the non-instrumentalist (who is concerned with evidence regarding the <i>truth</i> of competing theories). It would be nice to know how and why likelihoods are able to play this dual role.</p>  
<p>In contrast to Sober’s contribution to this volume, the papers of Howson, Dawid, and Earman adopt a Bayesian stance. The first part of Howson’s paper contains a wealth of historical, philosophical, and statistical wisdom. He discusses the role of Bayesian methods (and, by contrast, some of their most notable non-Bayesian rivals) in statistical theory and practice, beginning with the very first Bayesian methods used by Laplace (and Bayes himself), leading all the way up to the most recent foundational issues addressed by Bayesian statisticians and philosophers, including debates about “informationless” prior probabilities, and the importance of simplicity in hypothesis (or model) choice. Howson’s treatments of Fisherian and Neyman-Pearsonian statistical methods (and philosophies) are particularly informative and useful (the analogies with Popperian and hypothetico-deductive conceptions should be especially illuminating for philosophers). And, Howson’s discussion of Lindley’s Paradox is refreshing (it seems to me that not enough philosophical ink has been spilt over this important statistical conundrum).</p>  
<p>The second part of Howson’s paper (on which I will dwell a bit) is written from a more “logical” point of view. Here, he proposes a systematic, and general Bayesian (non-deductive, of course) “logic,” which is described in a way that makes it sound strongly analogous to (classical) <i>deductive</i> logic. He talks about “consistency” and “soundness” and “completeness”, <i>etc</i>. Some of the logicians among us will probably have deep worries about this analogy, and no doubt they will view use of this logical terminology as a non-trivial stretch. I must confess, I found myself feeling rather uncomfortable about the degree of force with which Howson pushes the analogy. I will focus here on Howson’s notion of “inconsistency,” but I think similar worries will apply to his other “logical” notions. When a (classical) logician talks about <i>inconsistency</i>, it is a notion that is <i>directly</i> relevant not only to decision-making and other (broadly) pragmatic disciplines, but also to <i>epistemology</i> (understood here in a traditional, <i>non-pragmatic</i> sense). It’s not entirely clear to me that Howson’s notion of “consistency” has such direct relevance to epistemology. Here, I am <i>not</i> worrying about the problems involving prior probabilities mentioned above. I am willing to <i>grant</i> (<i>arguendo</i>) that <i>they</i> <i>do</i> have epistemic and probative force. What I do not see is why someone who is “inconsistent” in Howson’s sense should feel any <i>epistemic</i> pressure to revise their degrees of belief. It seems logically consistent with Howson’s “inconsistency” that such an agent’s degrees of belief are <i>inter alia</i> as accurate as they have ever been (or ever will be).  This is (arguably) <i>not</i> the case when the agent’s beliefs are <i>logically</i> inconsistent. In that case, the agent <i>knows</i> there is something <i>false</i> in what they believe (and this is transparently a <i>bad thing</i>, from an epistemic point of view). What is the analogous thing that an “inconsistent” agent (in Howson’s sense) <i>knows</i> that would inspire them to change their degrees of belief? In this connection, it seems to me that Howson’s discussion is somewhat vague. He presents his “logic” without crucial details concerning the proofs of the key theorems that purport to forge the strong analogy between deductive logic and his Bayesian “logic”. For instance, Howson does not explain how the additivity axiom follows from his “consistency” assumptions (indeed, he even claims to establish the “inconsistency” of violations of <i>countable</i> additivity, which is <i>even more</i> controversial). This leaves one wondering whether the compelling objections to Dutch Book arguments that have been voiced by philosophers like Schick (1980) and Maher (1993) might have some bearing on Howson’s approach. Such philosophers seem to provide examples of cases in which it seems perfectly <i>rational</i> to violate the additivity axiom (and, therefore, Howson’s “consistency”). It would be nice to hear Howson explain what, precisely, makes such agent’s degrees of belief “bad” or “irrational” (in <i>any</i> compelling sense). More traditional logicians may want to have a look at Carnap’s (1950) insightful discussion of the relationship between deductive and inductive logic. Carnap’s inductive logic program may have failed, but its aim was to provide a notion of partial entailment that was <i>logical</i> in the <i>very same sense</i> (not merely in an <i>analogous</i> sense) that deductive logical consequence is <i>logical</i>, and thereby to <i>avoid</i> a pragmatic and/or subjective turn in inductive logic (which seems implicit – although now deeply buried –in Howson’s talk of “betting quotients” and “fairness”). It seems to me that this aim may still be achievable (albeit, probably in a non-Carnapian way), and until it is demonstrated that this goal cannot be achieved, perhaps it would make more sense to reserve the term “logic” for the non-pragmatic, non-contingent, and objective conception that traditional logicians have in mind. In the meantime, why not just stick with the term “rational”, as opposed to “logical” when characterizing Bayesian accounts of credence? Would anything really be lost?</p>   
<p>Dawid’s paper provides a very clear, simple, and sound introduction to the use of Bayesian theories of evidential support (and weighing evidence) in legal contexts. A fair amount of work has been done in this area over the past thirty years or so, and Dawid’s paper serves as a nice overview of the basic techniques that are applied by Bayesians in the context of legal evidence. One of the best things Dawid does is to make very clear the distinction between prior probabilities (degrees of <i>belief</i>) and likelihood ratios (degrees of <i>support</i> or <i>weight of evidence</i>). Many of the same issues discussed above in connection with Bayesian theories of evidential support arise in concrete and simple examples in Dawid’s paper. Dawid proposes the likelihood ratio measure <i>l</i>(<i>H</i>, <i>E</i>) = Pr(<i>E</i> | <i>H</i>) / Pr(<i>E</i> | ¬<i>H</i>) as the proper Bayesian measure of degree of support. This measure has been skillfully defended by I.J. Good for many years (see Good (1985)), and more recently has been shown to have various advantages over other Bayesian measures of confirmation (see Eells and Fitelson (2000), and Fitelson (2001)). Importantly, because of its sensitivity to the “catch-all” likelihood Pr(<i>E</i> | ¬<i>H</i>), <i>l</i> <i>violates</i> the strong “Law of Likelihood” discussed above (endorsed by Sober). And, yet, as Dawid’s examples illustrate, it often seems <i>crucial</i> to take account of such terms in our assessments (both contrastive and non-contrastive) of weight of evidence. In this sense, Dawid’s legal examples provide a nice testbed for clashing intuitions in the Bayes/non-Bayes controversy about evidential support. I think Dawid’s examples provide further reasons to worry about the legitimacy of the strong “Law of Likelihood,” and further reasons to retreat to Joyce’s (2003) <i>Weak</i> Law of Likelihood.</p>   
<p>Earman’s paper can be viewed as a sampler of a much longer essay he has written [Earman (2000)] on Hume’s arguments concerning miracles (an essay which I highly recommend, by the way). Earman provides a detailed historical trace of the arguments of Hume and his contemporaries concerning the possibility of compelling testimony about the occurrence of miracles. By carefully and skillfully applying Bayesian techniques to these arguments, Earman ends up with some very interesting (albeit somewhat anachronistic) new reconstructions of these infamous historical arguments. By and large, Earman’s reconstructions are accurate and novel, and his analyses are trenchant. His Bayesian treatment of multiple testimonial reports is especially illuminating. The only complaint I have about this paper is that it may focus too heavily on <i>posterior probabilities</i> Pr(<i>H</i> | <i>E</i>) of the various hypotheses <i>H</i> in question, given the various sorts of evidence <i>E</i> he considers. It would also be interesting to see parallel analyses done which focus more on the <i>likelihood ratios</i> <i>l</i>(<i>H</i>, <i>E</i>) that result in each of the reconstructions. I suspect the ensuing facts about <i>degree of support</i> would be harmonious with Earman’s conclusions about <i>degrees of belief</i> in these cases. But, examining things from the <i>weight of evidence</i> perspective (as Dawid does in the legal context) may shed further light on some of the issues and arguments. This is a minor complaint, and Earman is to be commended for the rich historical/philosophical tale he tells, and for the interesting applications of Bayesian machinery he musters.</p>  
<p>The final contemporary paper in this collection (aside from Swinburne’s solid introductory piece on which I have chosen not to comment explicitly) is Miller’s brief (but important) essay on the propensity interpretation of probability. Roughly, the propensity theory recommends interpreting Pr(<i>X</i> | <i>Y</i>) as the (presumably, <i>causal</i>) <i>propensity</i> <i>Y</i> has for bringing about <i>X</i> (usually, in some experimental context). Popper (1957) was one of the first to endorse a propensity interpretation of conditional probability, and many others have followed suit since. Humphreys (1985) pointed out that there seem to be deep problems with the existence and interpretation of the “inverse propensity” Pr(<i>Y</i> | <i>X</i>), since (presumably) <i>Y</i>’s having a causal propensity to bring about <i>X</i> does <i>not</i> imply <i>X</i>’s having a causal propensity to bring about <i>Y</i>. But, if “Pr” is to satisfy the probability axioms, then it must also satisfy <i>Bayes’s Theorem</i>, which would imply a perfectly well-defined and interpretable <i>inverse probability</i> Pr(<i>Y</i> | <i>X</i>). This became known as Humphreys’s Paradox. Many people came to believe that Humphreys had shown that propensities <i>cannot</i> satisfy the axioms of probability (or Bayes’s Theorem). [Indeed, it seems that some people already believed this before Humphreys’ paper appeared (see Fetzer and Nute (1980)).] In his contribution to the volume, David Miller shows that this is not the case. Indeed, Miller sketches a perfectly coherent and sensible propensity theory that is also a <i>probability theory</i>. As such, Miller shows how to diffuse Humphreys’s Paradox and restore the satisfaction of Bayes’s Theorem for propensities. As it turns out, there are various ways to mitigate Humphreys’ paradox in this sense. See Gillies (2001) for extended discussion of several approaches, including Miller’s.</p>  
<p>The volume closes with an Appendix containing a very polished reproduction of Bayes’s classic “An Essay Towards the Solving a Problem in the Doctrine of Chances”. The Essay still reads very well, and it should be on every probabilist’s “must read” list. I feel quite comfortable saying something almost as glowing about this entire volume. I found this book very edifying and clear, and the debates and issues it encompasses are of great importance for contemporary philosophy of probability, statistics, and decision-making. I highly recommend this book to anyone with interests in these areas, and I commend Swinburne for putting together this neat little book.<span style="font-weight: bold;">
</span></p> 
<p><span style="font-weight: bold;">References</span></p>      <p class="cit">Carnap, R., 1950, <i>Logical Foundations of Probability</i>, Chicago: University of Chicago Press.</p>  <p class="cit">Earman, J., 2000, Hume’s Abject Failure - The Argument Against Miracles, Oxford: Oxford University Press.</p>  <p class="cit">Eells, E. and Fitelson, B., 2002, “Symmetries and Asymmetries in Evidential Support,” <i>Philosophical Studies</i> 107: 129–142.</p>  <p class="cit">Fetzer, J. and Nute, D., 1980, “A Probabilistic Causal Calculus: Conflicting Conceptions,” <i>Synthese</i> 44: 241-246.</p>  <p class="cit">Fitelson, B., 1999, “The plurality of Bayesian measures of confirmation and the problem of measure sensitivity.” <i>Philosophy of Science</i> 66: S362–S378.</p>  <p class="cit">Fitelson, B., 2001, “A Bayesian Account of Independent Evidence with Applications,” <i>Philosophy of Science</i> 68: S123–S140.</p>  <p class="cit">Gillies, D., 2000, “Varieties of Propensity”, <i>British Journal for the Philosophy of Science</i> 51: 807–835.</p>  <p class="cit">Good, I. (1985). “Weight of evidence: A brief survey,” In <i>Bayesian Statistics, 2</i> (Valencia, 1983), pp. 249–269. Amsterdam: North-Holland. </p>  <p class="cit">Humphreys, P., 1985, “Why propensities cannot be probabilities,” <i>The Philosophical Review</i> 94: 557–570.</p>  <p class="cit">Joyce, J., 2003, “Bayes’ Theorem”, <i>The Stanford Encyclopedia of Philosophy</i> (Fall 2003 Edition), Edward N. Zalta (<i>ed</i>.), URL = http://plato.stanford.edu/entries/bayes-theorem/</p>  <p class="cit">Maher, P., 1993, <i>Betting on Theories</i>. Cambridge: Cambridge University Press.</p>  <p class="cit">Milne, P., 1996, “Log[<i>p</i>(<i>h/eb</i>)<i>/p</i>(<i>h/b</i>)] is the one true measure of confirmation,” <i>Philosophy of Science</i> 63: 21–26.</p>  <p class="cit">Popper, K., 1957, “The propensity interpretation of the calculus of probability, and the Quantum Theory,” in S. Körner (<i>ed</i>.): <i>Observation and Interpretation in the Philosophy of Physics</i>.</p>  <p class="cit">Rosenkrantz, R., 1977, <i>Inference, Method and Decision</i>. Dordrecht: D. Reidel.</p>  <p class="cit">Royall, R., 1997, <i>Statistical Evidence: A Likelihood Paradigm</i>. London: Chapman &amp; Hall.</p>  <p class="cit">Schick, F., 1986, “Dutch Bookies and Money Pumps,” <i>Journal of Philosophy</i> 83: 112–119.</p>  <p class="cit">Sober, E., 2003, “Likelihood and the Duhem/Quine Problem,” unpublished manuscript.</p>

  </div></div>

<div id="footer">
    <p>Copyright &copy; 2004 Notre Dame Philosophical Reviews<br />
  Site Last Modified: 3/8/2011  <br>
ISSN: 1538 - 1617</p>
    <p><a href="http://www.nd.edu/" id="ndhome">University of Notre Dame </a></p>
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-344381-6";
urchinTracker();
</script>
</div>

</div>
</body>
</html>
